services:
  translator_1_cuda0:
    image: translator:mg
    volumes:
      - ./tmp/tmp_01:/tmp
      - ./logs/logs_01:/app/logs
    environment:
      GPU_ID: 0
    ipc: host
    ports:
      - "1001:5050"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python api.py
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"
    networks:
      - loadbalance_net

  translator_2_cuda0:
    image: translator:mg
    volumes:
      - ./tmp/tmp_02:/tmp
      - ./logs/logs_02:/app/logs
    environment:
      GPU_ID: 0
    ipc: host
    ports:
      - "1002:5050"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python api.py
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"
    networks:
      - loadbalance_net

  translator_3_cuda0:
    image: translator:mg
    volumes:
      - ./tmp/tmp_03:/tmp
      - ./logs/logs_03:/app/logs
    environment:
      GPU_ID: 0
    ipc: host
    ports:
      - "1003:5050"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python api.py
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"
    networks:
      - loadbalance_net

  translator_4_cuda0:
    image: translator:mg
    volumes:
      - ./tmp/tmp_04:/tmp
      - ./logs/logs_04:/app/logs
    environment:
      GPU_ID: 0
    ipc: host
    ports:
      - "1004:5050"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python api.py
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"
    networks:
      - loadbalance_net

  translator_5_cuda0:
    image: translator:mg
    volumes:
      - ./tmp/tmp_05:/tmp
      - ./logs/logs_05:/app/logs
    environment:
      GPU_ID: 0
    ipc: host
    ports:
      - "1005:5050"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python api.py
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"
    networks:
      - loadbalance_net

  # translator_1_cuda1:
  #   image: translator:mg
  #   volumes:
  #     - ./tmp/tmp_11:/tmp
  #     - ./logs/logs_11:/app/logs
  #   environment:
  #     GPU_ID: 1
  #   ipc: host
  #   ports:
  #     - "1011:5050"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  #   command: python api.py
  #   logging:
  #     driver: "local"
  #     options:
  #       max-size: "100m"
  #       max-file: "14"
  #   networks:
  #     - loadbalance_net

  # translator_2_cuda1:
  #   image: translator:mg
  #   volumes:
  #     - ./tmp/tmp_12:/tmp
  #     - ./logs/logs_12:/app/logs
  #   environment:
  #     GPU_ID: 1
  #   ipc: host
  #   ports:
  #     - "1012:5050"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  #   command: python api.py
  #   logging:
  #     driver: "local"
  #     options:
  #       max-size: "100m"
  #       max-file: "14"
  #   networks:
  #     - loadbalance_net

  # translator_3_cuda1:
  #   image: translator:mg
  #   volumes:
  #     - ./tmp/tmp_13:/tmp
  #     - ./logs/logs_13:/app/logs
  #   environment:
  #     GPU_ID: 1
  #   ipc: host
  #   ports:
  #     - "1013:5050"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  #   command: python api.py
  #   logging:
  #     driver: "local"
  #     options:
  #       max-size: "100m"
  #       max-file: "14"
  #   networks:
  #     - loadbalance_net

  # translator_4_cuda1:
  #   image: translator:mg
  #   volumes:
  #     - ./tmp/tmp_14:/tmp
  #     - ./logs/logs_14:/app/logs
  #   environment:
  #     GPU_ID: 1
  #   ipc: host
  #   ports:
  #     - "1014:5050"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  #   command: python api.py
  #   logging:
  #     driver: "local"
  #     options:
  #       max-size: "100m"
  #       max-file: "14"
  #   networks:
  #     - loadbalance_net

  # translator_5_cuda1:
  #   image: translator:mg
  #   volumes:
  #     - ./tmp/tmp_15:/tmp
  #     - ./logs/logs_15:/app/logs
  #   environment:
  #     GPU_ID: 1
  #   ipc: host
  #   ports:
  #     - "1015:5050"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['1']
  #             capabilities: [gpu]
  #   command: python api.py
  #   logging:
  #     driver: "local"
  #     options:
  #       max-size: "100m"
  #       max-file: "14"
  #   networks:
  #     - loadbalance_net

  nginx_translator:
    image: nginx_load_balancer:mg
    volumes:
      - ./tmp/tmp_nginx:/tmp
      - ./logs/logs_nginx:/app/logs
    ports:
      - "2486:80"
    networks:
      - loadbalance_net
    logging:
      driver: "local"
      options:
        max-size: "100m"
        max-file: "14"

networks:
  loadbalance_net:
    driver: bridge